{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericslevenson/arctic-surface-water/blob/main/Copy_of_basinTimeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbIk1t_vtnky"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "author: @ericslevenson\n",
        "date: Apr 27, 2024\n",
        "description: Time series of lake extent within watersheds across the Arctic\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1sOBDWftkcr"
      },
      "outputs": [],
      "source": [
        "# Authenticate private account (only required for exporting to drive/gee/gcp)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Earth Engine setup\n",
        "import ee # Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='ee-eric-levenson') # Initialize the library.\n",
        "\n",
        "# Google Drive setup (if needed)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Some common imports\n",
        "from IPython.display import Image\n",
        "import folium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids=[3423047021, 3423046031, 3423042503, 3423047050, 3421808022, 3423046032, 3421808042, 3421809033, 3423047060, 3423047071, 3421808070, 3424121020, 3424110220, 3423060301, 3423042650, 3423042641, 3423047092, 3424121071, 3424110271, 3423042910, 3423047093, 3423042643, 3423042642, 3423048001, 3424121080, 3424121091, 3423049030, 3423042691, 3423042680, 3423042662, 3423042950, 3423042941, 3423049060, 3423042693, 3423042692, 3424121092, 3423060430, 3424110273, 3423048002, 3423042862, 3423049080, 3423048005, 3424122020, 3423048004, 3424140170, 3424124401, 3424140502, 3424140601, 3424124212, 3424125015, 3424152212, 3424152232, 3424152305, 3424125032, 3424154601, 3424154700, 3424154231, 3422220950, 3425118082, 3423089071, 3424124802, 3425118092, 3423089073, 3423089072, 3424152502, 3424129000, 3424174021, 3424176221, 3425300142, 3425300201, 3424174070, 3425160056, 3424163001, 3425300500, 3425300910, 3424176271, 3424156940, 3424176261, 3425410020, 3422259202, 3425410063, 3424181500, 3425410062, 3422369013, 3425502070, 3422260360, 3425410042, 3422540412, 3425440103, 3425440102, 3425502062, 3425502063, 3422369015, 3425505040, 3424181232, 3424354203, 3424355002, 3424322012, 3422260395, 3422270060, 3424213721, 3425420502, 3425504030, 3424357031, 3422280100, 3422291011, 3424323060, 3424323070, 3425420506, 3425420507, 3422571502, 3424181235, 3424357032, 3425440601, 3425420701, 3425440203, 3422434401, 3425420253, 3425420702, 3425420705, 3425420704, 3422434502, 3425506060, 3422291050, 3425420254, 3425420901, 3425506080, 3425440602, 3424324023, 3424322090, 3425420805, 3424322080, 3425420804, 3425440902, 3425460360, 3422280803, 3422280802, 3424342003, 3424342002, 3422572110, 3422526003, 3422560911, 3422280971, 3424182350, 3425508093, 3424360311, 3422572130, 3424344101, 3422560803, 3422528003, 3422560960, 3425460802, 3422295030, 3422436073, 3425460803, 3422295020, 3424182603, 3424182602, 3422560991, 3422439060, 3422440104, 3422297091, 3422529072, 3422572901, 3424182832, 3424376202, 3424186533, 3424376550, 3425704403, 3424387070, 3424389111, 3424389113, 3424388202, 3424389133, 3424389150, 3424389161, 3424389204, 3424389910, 3424389205, 3422930601, 3511081640, 3511086170, 3511090053, 3511090075, 3512140602, 3512804245, 3512804244, 3513100303, 3513100500, 3513201030, 3513205002, 3513800402, 3513900421, 3513900701, 3513900702, 3513900703, 3513900900, 3514120910, 3514150906, 3514180210, 3514160910, 3514160801, 3514160931, 3514520704, 3514520630, 3514520621, 3514550350, 3514520640, 3514520651, 3514550370, 3514520932, 3514520933, 3514520950, 3514520941, 3514520922, 3514520802, 3514520803, 3514268021, 3514302802, 3514550707, 3514560161, 3514405101, 3514404100, 3514560441, 3514701010, 3514404700, 3514580403, 3514580402, 3514560262, 3514610210, 3514560812, 3514610770, 3514630310, 3514704403, 3514630204, 3514704663, 3514990434, 3515000100, 3516030801, 3516050704, 3516060601, 3516028062, 3517010462, 3517090010, 3519100303, 3519202033, 3519303035, 3519702100, 3519705030, 3519807013, 3519806604, 3530118220, 3530118940, 3530132500, 3541100100, 3541241602, 3541442721, 3541498952, 3541500050, 3541500060, 3541500070, 3541610101, 3541610106, 3541610401, 3541610501, 3541610402, 3541610405, 3541610502, 3541610503, 3541610701, 3541610601, 3541610702, 3541610603, 3541630027, 3541650003, 3541650002, 3541650004, 3541650005, 3541620082, 3541660301, 3541660240, 3541640092, 3541670701, 3541670703, 3541691023, 3541691060, 3541680030, 3541680020, 3541694001, 3541697000, 3541710203, 3541710301, 3541710302, 3541710303, 3541710434, 3541710701, 3541710702, 3541710703, 3541710800, 3541710900, 3541721001, 3541724692, 3541750102, 3541750103, 3541750205, 3541750206, 3541750300, 3541750410, 3541750550, 3541802042, 3541809304, 3541806063, 3541808080, 3541806073, 3541808090, 3541809510, 3541809430, 3545046502, 3545047543, 3545047420, 3545046470, 3545046460, 3545046920, 3545046931, 3545046803, 3545046802, 3545049603, 3545048802, 3560110629, 3560110647, 3560110646, 3560110672, 3560110675, 3560110674, 3560110662, 3560110681, 3560122053, 3560122091, 3560123063, 3560122093, 3560124003, 3560125050, 3560125041, 3560127000, 3560126021, 3560126051, 3560126093, 3560129500, 3560131204, 3560131501, 3560131504, 3560131505, 3560131601, 3560131800, 3560131900, 3560132012, 3560135051, 3560135052, 3560135070, 3560136010, 3560136020, 3560138603, 3560138901, 3560138802, 3560144082, 3560145004, 3560148024, 3560156802, 3560157110, 3560169302, 3560169202, 3560218490, 3560244060, 3560242281, 3560242291, 3560242282, 3560252730, 3560252720, 3560252612, 3560252750, 3560252742, 3560252761, 3560252630, 3560264051, 3560262060, 3560265040, 3560265050, 3560264052, 3560262081, 3560262091, 3560264042, 3560266093, 3560272243, 3560272400, 3560272512, 3560272513, 3560272285, 3560272284, 3320020003, 3320020005, 3320031001, 3320031002, 3320031003, 3320031005, 3320039330, 3320039350, 3320039360, 3320039403, 3320039500, 3320039752, 3320039753, 3320039800, 3320071050, 3320073051, 3320073052, 3320075300, 3320077020, 3320077030, 3320077050, 3320080001, 3320080002, 3317000010, 3317000051, 3317000052, 3317000053, 3317000060, 3317000070, 3317000080, 3317000090, 3520010350, 3520010402, 3520010403, 3520010622, 3520010662, 3520010701, 3520010702, 3520010801, 3520010803, 3520010901, 3520020010, 3520020030, 3520020040, 3520031111, 3520031112, 3520031310, 3520031320, 3520031330, 3520031720, 3520031730, 3520031770, 3520031801, 3520031920, 3520031930, 3520031941, 3520031942, 3520031991, 3520031992, 3520031993, 3520032011, 3520032012, 3520032030, 3520032020, 3520032091, 3520032081, 3520032083, 3520032082, 3520033020, 3520033061, 3520033063, 3520033062, 3520033070, 3520034100, 3520034200, 3520034303, 3520034500, 3520034702, 3520034703, 3520034612, 3520034613, 3520034630, 3520034623, 3520034800, 3520035011, 3520035012, 3520035023, 3520036010, 3520036043, 3520037030, 3520038100, 3520038310, 3520038330, 3520038320, 3520038370, 3520038502, 3520038700, 3520039010, 3520039021, 3520039070, 3520050000, 3520060350, 3520060360, 3520060380, 3520060370, 3520060410, 3520060430, 3520060421, 3520060440, 3520060450, 3520060705, 3520070000, 3517000000, 3519000010, 3519000030, 3519000040, 3519000060, 3519000070, 3519000080, 3530200110, 3530200120, 3530200130, 3530200140, 3530200150, 3530200170, 3530200180]"
      ],
      "metadata": {
        "id": "XrC8o8yPDsLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPWtTvc3t7B_"
      },
      "outputs": [],
      "source": [
        "## ***INPUTS***\n",
        "\n",
        "## Regularly Adjusted Inputs ##\n",
        "\n",
        "# Tile of Interest ID\n",
        "\n",
        "start = '2016-05-01'\n",
        "finish = '2023-10-01'\n",
        "\n",
        "## Periodically Adjusted Inputs\n",
        "# Lake Shapefiles\n",
        "lakes = ee.FeatureCollection('projects/ee-eric-levenson/assets/PLD/PLD_60n0e_buff')\n",
        "lakes = lakes.merge(ee.FeatureCollection('projects/ee-eric-levenson/assets/PLD/PLD_60n110w_buff'))\n",
        "lakes = lakes.merge(ee.FeatureCollection('projects/ee-eric-levenson/assets/PLD/PLD_60n180w_buff'))\n",
        "lakes = lakes.merge(ee.FeatureCollection('projects/ee-eric-levenson/assets/PLD/PLD_60n60w_buff'))\n",
        "lakes = lakes.merge(ee.FeatureCollection('projects/ee-eric-levenson/assets/PLD/PLD_60n90e_buff'))\n",
        "\n",
        "# Export settings\n",
        "directory = 'timeSeries_reprocessed' #Export Folder\n",
        "description_ending = f'_timeSeries' # Export Name after tile ID\n",
        "\n",
        "#Basins\n",
        "basin_l10 = ee.FeatureCollection(\"projects/sat-io/open-datasets/HydroAtlas/BasinAtlas/BasinATLAS_v10_lev10\")\n",
        "\n",
        "# exports\n",
        "exportSelectors = ['id', 'date', 'waterArea', 'coverage']\n",
        "\n",
        "## Rarely Adjusted Inputs ##\n",
        "# Image scale\n",
        "pixScale = 10\n",
        "# Cloud probability threshold\n",
        "CLD_PRB_THRESH = 50\n",
        "## ***EARTH ENGINE-IFY***\n",
        "startDoy = ee.Date(start).getRelative('day', 'year')\n",
        "endDoy = ee.Date(finish).getRelative('day', 'year')\n",
        "eestart = ee.Date(start)\n",
        "eefinish = ee.Date(finish)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaTU-Kdo4ED-"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDpGtmCJ4HW6"
      },
      "source": [
        "### Image Pre-Processing Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2lVJ3UO4K3n"
      },
      "outputs": [],
      "source": [
        "## ***IMAGE PRE-PROCESSING METHODS***\n",
        "\n",
        "# Mask clouds in Sentinel-2\n",
        "def add_cloud_bands(img):\n",
        "    # Get s2cloudless image, subset the probability band.\n",
        "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
        "\n",
        "    # Condition s2cloudless by the probability threshold value.\n",
        "    clouds = cld_prb.gte(CLD_PRB_THRESH).rename('cloud_mask')\n",
        "    clear = cld_prb.lt(CLD_PRB_THRESH).rename('clear_mask')\n",
        "    # Add the cloud probability layer and cloud mask as image bands.\n",
        "    return img.addBands(ee.Image([cld_prb, clouds, clear]))\n",
        "\n",
        "# Clip image\n",
        "def clip_image(image):\n",
        "  '''Clips to the roi defined at the beginning of the script'''\n",
        "  return image.clip(roi)\n",
        "\n",
        "def clip2lakes(image):\n",
        "  '''Clips an image based on the lake boundaries'''\n",
        "  return image.clip(lakes)\n",
        "\n",
        "# Get percentile cover\n",
        "def getCover(image):\n",
        "  '''calculates percentage of the roi covered by the clear mask. NOTE: this function\n",
        "  calls the global totPixels variable that needs to be calculated in the main script.'''\n",
        "  actPixels = ee.Number(image.updateMask(image.select('clear_mask')).reduceRegion(\n",
        "      reducer = ee.Reducer.count(),\n",
        "      scale = 1000, # keep same as totPixels\n",
        "      geometry = image.geometry(),\n",
        "      maxPixels=1e12,\n",
        "      ).values().get(0))\n",
        "  # calculate the perc of cover OF CLEAR PIXELS\n",
        "  percCover = actPixels.divide(totPixels).multiply(100).round()\n",
        "  # number as output\n",
        "  return image.set('percCover', percCover,'actPixels',actPixels)\n",
        "\n",
        "# Mosaic images by date, orbit, - basically combines images together that were taken on the same day\n",
        "def mosaicBy(imcol):\n",
        "  '''Takes an image collection (imcol) and creates a mosaic for each day\n",
        "  Returns: An image collection of daily mosaics'''\n",
        "  #return the collection as a list of images (not an image collection)\n",
        "  imlist = imcol.toList(imcol.size())\n",
        "  # Get all the dates as list\n",
        "  def imdate(im):\n",
        "    date = ee.Image(im).date().format(\"YYYY-MM-dd\")\n",
        "    return date\n",
        "  all_dates = imlist.map(imdate)\n",
        "  # get all orbits as list\n",
        "  def orbitId(im):\n",
        "    orb = ee.Image(im).get('SENSING_ORBIT_NUMBER')\n",
        "    return orb\n",
        "  all_orbits = imlist.map(orbitId)\n",
        "  # get all spacecraft names as list\n",
        "  def spacecraft(im):\n",
        "    return ee.Image(im).get('SPACECRAFT_NAME')\n",
        "  all_spNames = imlist.map(spacecraft)\n",
        "  # this puts dates, orbits and names into a nested list\n",
        "  concat_all = all_dates.zip(all_orbits).zip(all_spNames);\n",
        "  # here we unnest the list with flatten, and then concatenate the list elements with \" \"\n",
        "  def concat(el):\n",
        "    return ee.List(el).flatten().join(\" \")\n",
        "  concat_all = concat_all.map(concat)\n",
        "  # here, just get distinct combintations of date, orbit and name\n",
        "  concat_unique = concat_all.distinct()\n",
        "  # mosaic\n",
        "  def mosaicIms(d):\n",
        "    d1 = ee.String(d).split(\" \")\n",
        "    date1 = ee.Date(d1.get(0))\n",
        "    orbit = ee.Number.parse(d1.get(1)).toInt()\n",
        "    spName = ee.String(d1.get(2))\n",
        "    im = imcol.filterDate(date1, date1.advance(1, \"day\")).filterMetadata('SPACECRAFT_NAME', 'equals', spName).filterMetadata('SENSING_ORBIT_NUMBER','equals', orbit).mosaic()\n",
        "    return im.set(\n",
        "        \"system:time_start\", date1.millis(),\n",
        "        \"system:date\", date1.format(\"YYYY-MM-dd\"),\n",
        "        \"system:id\", d1)\n",
        "  mosaic_imlist = concat_unique.map(mosaicIms)\n",
        "  return ee.ImageCollection(mosaic_imlist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsNqzzQYNUEP"
      },
      "source": [
        "### Ice Classification Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiD-YhiyNSjC"
      },
      "outputs": [],
      "source": [
        "def ice_classify(image):\n",
        "    clear_mask = image.select('clear_mask')\n",
        "    ice = image.select('B4').gte(1000).rename('ice')  # Addy's threshold\n",
        "    ice = ice.updateMask(clear_mask)  # Apply clear mask to ice classification\n",
        "    #all = image.select('B4').gte(1).rename('all')\n",
        "    return image.addBands([ice])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHFNOQSX4R58"
      },
      "source": [
        "### Water Classification Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAkjlAWa4U67"
      },
      "outputs": [],
      "source": [
        "###########################################################################\n",
        "## ***WATER CLASSIFICATION METHODS***\n",
        "\n",
        "# Define NDWI image\n",
        "def ndwi(image):\n",
        "  '''Adds an NDWI band to the input image'''\n",
        "  return image.normalizedDifference(['B3', 'B8']).rename('NDWI').multiply(1000)\n",
        "\n",
        "# Basic ndwi classification\n",
        "def ndwi_classify(image):\n",
        "  '''Creates a binary image based on an NDWI threshold of 0'''\n",
        "  ndwimask = image.select('NDWI')\n",
        "  water = ndwimask.gte(0)\n",
        "  land = ndwimask.lt(0)\n",
        "  return(water)\n",
        "\n",
        "\n",
        "def ndwiMean(image):\n",
        "  '''calculate NDWI histogram and add mean as a property to the image. Used to filter out images with empty histograms!'''\n",
        "  NDWI = ndwi(image).select('NDWI').updateMask(image.select('clear_mask'))\n",
        "  ndwimean = ee.Dictionary(NDWI.reduceRegion(\n",
        "    geometry = roi,\n",
        "    reducer = ee.Reducer.histogram(255, 2).combine('mean', None, True).combine('variance', None, True),\n",
        "    scale = 100,\n",
        "    maxPixels = 1e12\n",
        "    )).get('NDWI_mean')\n",
        "  return image.set('ndwiMean', ndwimean)\n",
        "\n",
        "\n",
        "# OTSU thresholding from histogram\n",
        "def otsu(histogram):\n",
        "  '''Returns the NDWI threshold for binary water classification'''\n",
        "  counts = ee.Array(ee.Dictionary(histogram).get('histogram'))\n",
        "  means = ee.Array(ee.Dictionary(histogram).get('bucketMeans'))\n",
        "  size = means.length().get([0])\n",
        "  total = counts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
        "  sum = means.multiply(counts).reduce(ee.Reducer.sum(), [0]).get([0])\n",
        "  mean = sum.divide(total)\n",
        "  indices = ee.List.sequence(1, size)\n",
        "  def func_xxx(i):\n",
        "    '''Compute between sum of squares, where each mean partitions the data.'''\n",
        "    aCounts = counts.slice(0, 0, i)\n",
        "    aCount = aCounts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
        "    aMeans = means.slice(0, 0, i)\n",
        "    aMean = aMeans.multiply(aCounts) \\\n",
        "        .reduce(ee.Reducer.sum(), [0]).get([0]) \\\n",
        "        .divide(aCount)\n",
        "    bCount = total.subtract(aCount)\n",
        "    bMean = sum.subtract(aCount.multiply(aMean)).divide(bCount)\n",
        "    return aCount.multiply(aMean.subtract(mean).pow(2)).add(\n",
        "           bCount.multiply(bMean.subtract(mean).pow(2)))\n",
        "  bss = indices.map(func_xxx)\n",
        "  # Return the mean value corresponding to the maximum BSS.\n",
        "  return means.sort(bss).get([-1])\n",
        "\n",
        "# OTSU thresholding for an image\n",
        "def otsu_thresh(water_image):\n",
        "  '''Calculate NDWI and create histogram. Return the OTSU threshold.'''\n",
        "  NDWI = ndwi(water_image).select('NDWI').updateMask(water_image.select('clear_mask'))\n",
        "  histogram = ee.Dictionary(NDWI.reduceRegion(\n",
        "    geometry = roi,\n",
        "    reducer = ee.Reducer.histogram(255, 2).combine('mean', None, True).combine('variance', None, True),\n",
        "    scale = 10,\n",
        "    maxPixels = 1e12\n",
        "  ))\n",
        "  return otsu(histogram.get('NDWI_histogram'))\n",
        "\n",
        "# Classify an image using OTSU threshold.\n",
        "def otsu_classify(water_image):\n",
        "  '''(1) Calculate NDWI and create histogram. (2) Calculate NDWI threshold for\n",
        "  binary classification using OTSU method. (3) Classify image and add layer to input image.\n",
        "  '''\n",
        "  NDWI = ndwi(water_image).select('NDWI')\n",
        "  histogram = ee.Dictionary(NDWI.reduceRegion(\n",
        "    geometry = roi,\n",
        "    reducer = ee.Reducer.histogram(255, 2).combine('mean', None, True).combine('variance', None, True),\n",
        "    scale = pixScale,\n",
        "    maxPixels = 1e12\n",
        "  ))\n",
        "  threshold = otsu(histogram.get('NDWI_histogram'))\n",
        "  otsu_classed = NDWI.gt(ee.Number(threshold)).And(water_image.select('B8').lt(2000)).rename('otsu_classed')\n",
        "  return water_image.addBands([otsu_classed])\n",
        "\n",
        "def adaptive_thresholding(water_image):\n",
        "  '''Takes an image clipped to lakes and returns the water mask'''\n",
        "  NDWI = ndwi(water_image).select('NDWI').updateMask(water_image.select('clear_mask')) # get NDWI **TURNED OFF CLOUD MASK, because i do it in the mains cript\n",
        "  threshold = ee.Number(otsu_thresh(water_image))\n",
        "  threshold = threshold.divide(10).round().multiply(10)\n",
        "  # get fixed histogram\n",
        "  histo = NDWI.reduceRegion(\n",
        "      geometry = roi,\n",
        "      reducer = ee.Reducer.fixedHistogram(-1000, 1000, 200),\n",
        "      scale = 10, # This was 30, keep at 10!?!?\n",
        "      maxPixels = 1e12\n",
        "  )\n",
        "  hist = ee.Array(histo.get('NDWI'))\n",
        "  counts = hist.cut([-1,1])\n",
        "  buckets = hist.cut([-1,0])\n",
        "  #find split points from otsu threshold\n",
        "  threshold = ee.Array([threshold]).toList()\n",
        "  buckets_list = buckets.toList()\n",
        "  split = buckets_list.indexOf(threshold)\n",
        "  # split into land and water slices\n",
        "  land_slice = counts.slice(0,0,split)\n",
        "  water_slice = counts.slice(0,split.add(1),-1)\n",
        "  # find max of land and water slices\n",
        "  land_max = land_slice.reduce(ee.Reducer.max(),[0])\n",
        "  water_max = water_slice.reduce(ee.Reducer.max(),[0])\n",
        "  land_max = land_max.toList().get(0)\n",
        "  water_max = water_max.toList().get(0)\n",
        "  land_max = ee.List(land_max).getNumber(0)\n",
        "  water_max = ee.List(water_max).getNumber(0)\n",
        "  #find difference between land, water and otsu val\n",
        "  counts_list = counts.toList()\n",
        "  otsu_val = ee.Number(counts_list.get(split))\n",
        "  otsu_val = ee.List(otsu_val).getNumber(0)\n",
        "  land_prom = ee.Number(land_max).subtract(otsu_val)\n",
        "  water_prom = ee.Number(water_max).subtract(otsu_val)\n",
        "  #find land and water buckets corresponding to 0.9 times the prominence\n",
        "  land_thresh = ee.Number(land_max).subtract((land_prom).multiply(ee.Number(0.9)))\n",
        "  water_thresh = ee.Number(water_max).subtract((water_prom).multiply(ee.Number(0.9)))\n",
        "  land_max_ind = land_slice.argmax().get(0)\n",
        "  water_max_ind = water_slice.argmax().get(0)\n",
        "  li = ee.Number(land_max_ind).subtract(1)\n",
        "  li = li.max(ee.Number(1))\n",
        "  wi = ee.Number(water_max_ind).add(1)\n",
        "  wi = wi.min(ee.Number(199))\n",
        "  land_slice2 = land_slice.slice(0,li,-1).subtract(land_thresh)\n",
        "  water_slice2 = water_slice.slice(0,0,wi).subtract(water_thresh)\n",
        "  land_slice2  = land_slice2.abs().multiply(-1)\n",
        "  water_slice2 = water_slice2.abs().multiply(-1)\n",
        "  land_index = ee.Number(land_slice2.argmax().get(0)).add(land_max_ind)\n",
        "  water_index = ee.Number(water_slice2.argmax().get(0)).add(split)\n",
        "  land_level = ee.Number(buckets_list.get(land_index))\n",
        "  water_level = ee.Number(buckets_list.get(water_index))\n",
        "  land_level = ee.Number(ee.List(land_level).get(0)).add(5)\n",
        "  water_level = ee.Number(ee.List(water_level).get(0)).add(5)\n",
        "  #calculate water fraction and classify\n",
        "  water_fraction = (NDWI.subtract(land_level)).divide(water_level.subtract(land_level)).multiply(100).rename('water_fraction')\n",
        "  #water_fraction = conditional(water_fraction) #sets values less than 0 to 0 and greater than 100 to 100\n",
        "  water_75 = water_fraction.gte(75).rename('water_75'); #note, this is a non-binary classification, so we use 75% water as \"water\"\n",
        "  all_mask = water_image.select('B2').gt(5).rename('all_mask')\n",
        "  cloud_mask_ed = water_image.select('cloud_mask').rename('cloud_mask_ed')\n",
        "  return water_image.addBands([water_fraction,water_75,NDWI,cloud_mask_ed])\n",
        "\n",
        "def binaryImage(image):\n",
        "  '''takes a multiband image and returns just the binary water_75 band'''\n",
        "  img = image.select('water_75')\n",
        "  return img\n",
        "def waterImage(image):\n",
        "  '''takes a multiband image and returns just the water fraction band'''\n",
        "  img = image.select('water_fraction')\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvKNz9QK4nFp"
      },
      "source": [
        "### Property Extraction Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFWGFfbSfQsq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pixelArea(image):\n",
        "  areaIm = image.pixelArea()\n",
        "  lakeIm = image.addBands([areaIm])\n",
        "  return lakeIm\n",
        "\n",
        "def sumWater(image):\n",
        "  '''sums the water pixels within the watershed image and adds the result to the feature'''\n",
        "  waterAreaIm = image.updateMask(image.select('water_75')) # mask area image based on water\n",
        "  # calculate the total area\n",
        "  watersum = waterAreaIm.select('area').reduceRegion(\n",
        "      reducer=ee.Reducer.sum(),\n",
        "      geometry = image.geometry(),\n",
        "      scale = 10,\n",
        "      maxPixels=1e9\n",
        "  ).get('area')\n",
        "  return watersum\n",
        "\n",
        "def sumIce(image):\n",
        "  '''sums the ice pixels within the watershed image and adds the result to the feature'''\n",
        "  iceAreaIm = image.updateMask(image.select('ice')) # mask area image based on ice\n",
        "  # calculate the total area\n",
        "  icesum = iceAreaIm.select('area').reduceRegion(\n",
        "      reducer=ee.Reducer.sum(),\n",
        "      geometry = image.geometry(),\n",
        "      scale = 100,\n",
        "      maxPixels=1e9\n",
        "  ).get('area')\n",
        "  return icesum\n",
        "\n",
        "def getClearArea(image):\n",
        "  '''sums the clear pixels within the watershed image and adds the result to the feature'''\n",
        "  clearAreaIm = image.updateMask(image.select('clear_mask')) # mask area image based on clearness\n",
        "  clearArea = clearAreaIm.select('area').reduceRegion(\n",
        "      reducer=ee.Reducer.sum(),\n",
        "      geometry = image.geometry(),\n",
        "      scale = 10,\n",
        "      maxPixels=1e9\n",
        "  ).get('area')\n",
        "  return clearArea\n",
        "\n",
        "def getCloudArea(image):\n",
        "  '''sums the cloud pixels within the watershed image and adds the result to the feature'''\n",
        "  cloudAreaIm = image.updateMask(image.select('cloud_mask')) # mask area image based on clouds\n",
        "  cloudArea = cloudAreaIm.select('area').reduceRegion(\n",
        "      reducer=ee.Reducer.sum(),\n",
        "      geometry = image.geometry(),\n",
        "      scale = 10, ## TODO: I updated this scale from 10 to 100, does that change results?\n",
        "      maxPixels=1e9\n",
        "  ).get('area')\n",
        "  return cloudArea\n",
        "\n",
        "def dayProps(image):\n",
        "  '''Input: classified water Image --> Output: Feature with properties such as date, water area, clear area, etc.\n",
        "  Map this function to the image collection of classified lake area images. This function calls\n",
        "  methods to calculate the water area, clear area, and cloud area. '''\n",
        "  basin = id #TODO: FIXME\n",
        "  date = image.date().format('yyyy-MM-dd')\n",
        "  water = sumWater(image)\n",
        "  #ice = sumIce(image)\n",
        "  cover = image.get('percCover')\n",
        "  #iceCover = image.get('percIceCover')\n",
        "  #clear = getClearArea(image)\n",
        "  #cloud = getCloudArea(image)\n",
        "  return ee.Feature(None, {'id': basin, 'date': date, 'waterArea': water,  'coverage':cover})\n",
        "\n",
        "def export_lakes(collection, description, fileNamePrefix, fileFormat, folder, selectors):\n",
        "  '''Export a feature collection of lake properties to google drive for a given day.'''\n",
        "  task = ee.batch.Export.table.toDrive(**{\n",
        "    'collection': collection,\n",
        "    'description': description,\n",
        "    'fileNamePrefix': fileNamePrefix,\n",
        "    'fileFormat': fileFormat,\n",
        "    'folder': folder,\n",
        "    'selectors': selectors\n",
        "  })\n",
        "  task.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI1I7Qhy5Pwh"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojCSfz4Et76N"
      },
      "outputs": [],
      "source": [
        "## ***MAIN***\n",
        "for i, id in enumerate(ids):\n",
        "  #(1) define roi\n",
        "  basin = ee.Feature(basin_l10.filter(ee.Filter.eq('PFAF_ID', id)).first())\n",
        "  roi = ee.Geometry.MultiPolygon(basin.geometry().getInfo()['coordinates'][0]) # define roi as geometry variable\n",
        "\n",
        "  # (2) image pre-process:\n",
        "  # Get images and filter images by cloudiness, roi, time period, and month range\n",
        "  images = ee.ImageCollection('COPERNICUS/S2_HARMONIZED').filterBounds(roi).filterDate(start,finish).filter(ee.Filter.calendarRange(startDoy, endDoy, 'day_of_year')).filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',60)) # Get Images\n",
        "  # Get cloud probability image collection\n",
        "  s2Cloudless = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY').filterBounds(roi).filterDate(start,finish).filter(ee.Filter.calendarRange(startDoy, endDoy, 'day_of_year'))\n",
        "  # Merge surface reflectance and cloud probability collections\n",
        "  images = ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
        "          'primary': images,\n",
        "          'secondary': s2Cloudless,\n",
        "          'condition': ee.Filter.equals(**{\n",
        "              'leftField': 'system:index',\n",
        "              'rightField': 'system:index'\n",
        "          })\n",
        "      }))\n",
        "  images = images.map(add_cloud_bands)\n",
        "  # Mosaic images and add cloud/clear masks\n",
        "  images_all = mosaicBy(images)\n",
        "  # Clip mosaics to roi\n",
        "  images_all = images_all.map(clip_image)\n",
        "  # Clip remaining mosaics to buffered lake shapefile\n",
        "  lakeimages = images_all.map(clip2lakes) # Clip images to buffered lake mask\n",
        "\n",
        "  # Get percent cover for each mosaic\n",
        "  image_mask = lakeimages.select('B2').mean().gte(0) #\n",
        "  # Calculate total number of pixels\n",
        "  totPixels = ee.Number(image_mask.reduceRegion(\n",
        "      reducer = ee.Reducer.count(),\n",
        "      scale = 1000,\n",
        "      geometry = roi,\n",
        "      maxPixels = 1e12\n",
        "      ).values().get(0))\n",
        "  lakeimages = lakeimages.map(getCover) # add percentage cover as an image property\n",
        "  # Filter by percent cover\n",
        "  lakeimages = lakeimages.filterMetadata('percCover','greater_than',70) # remove images covering less than 70% of the ROI)\n",
        "  lakeimages = lakeimages.map(ndwiMean) #\n",
        "  lakeimages = lakeimages.filter(ee.Filter.notNull(['ndwiMean']))\n",
        "  # (3) Classify water\n",
        "  lakeimages = lakeimages.map(adaptive_thresholding)\n",
        "  #lakeimages = lakeimages.map(ice_classify)\n",
        "  # Convert to area images\n",
        "  lakeimages = lakeimages.map(pixelArea) # get a pixel area image\n",
        "  #(4) Calculate water and ice area, convert to table format\n",
        "  days = lakeimages.map(dayProps)\n",
        "\n",
        "  #(5) Export\n",
        "  export_lakes(days, str(id), str(id), 'csv', directory, exportSelectors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GdBR4Qv5S9T"
      },
      "source": [
        "## Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w8tMHLH5X1U"
      },
      "outputs": [],
      "source": [
        "id=8100140540\n",
        "basin = ee.Feature(basin_l10.filter(ee.Filter.eq('HYBAS_ID', id)).first())\n",
        "roi = ee.Geometry.Polygon(basin.geometry().getInfo()['coordinates'][0]) # define roi as geometry variable\n",
        "\n",
        "  # (2) image pre-process:\n",
        "  # Get images and filter images by cloudiness, roi, time period, and month range\n",
        "imagesv2 = ee.ImageCollection('COPERNICUS/S2_HARMONIZED').filterBounds(roi).filterDate(start,finish).filter(ee.Filter.calendarRange(startDoy, endDoy, 'day_of_year')).filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',50)) # Get Images\n",
        "s2Cloudless = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY').filterBounds(roi).filterDate(start,finish).filter(ee.Filter.calendarRange(startDoy, endDoy, 'day_of_year'))\n",
        "ims = ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
        "        'primary': imagesv2,\n",
        "        'secondary': s2Cloudless,\n",
        "        'condition': ee.Filter.equals(**{\n",
        "            'leftField': 'system:index',\n",
        "            'rightField': 'system:index'\n",
        "        })\n",
        "    }))\n",
        "\n",
        "ims = ims.map(add_cloud_bands)\n",
        "images_allv2 = mosaicBy(ims)\n",
        "images_allv2 = images_allv2.map(clip_image)\n",
        "image_maskv2 = images_allv2.select('B2').mean().clip(roi).gte(0) #First, calculate total number of pixels\n",
        "totPixels = ee.Number(image_maskv2.reduceRegion(\n",
        "    reducer = ee.Reducer.count(),\n",
        "    scale = 100,\n",
        "    geometry = roi,\n",
        "    maxPixels = 1e12\n",
        "    ).values().get(0))\n",
        "images_allv2 = images_allv2.map(getCover)\n",
        "#images_allv2 = images_allv2.map(applyMask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f17RmLM231f1"
      },
      "outputs": [],
      "source": [
        "lakeimages = images_allv2.map(clip2lakes) # Clip images to buffered lake mask\n",
        "\n",
        "  # (3) Classify water\n",
        "lakeimages = lakeimages.map(adaptive_thresholding)\n",
        "  # Convert to area images"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YaTU-Kdo4ED-",
        "wDpGtmCJ4HW6",
        "GsNqzzQYNUEP",
        "mHFNOQSX4R58",
        "-GdBR4Qv5S9T"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPBbly06HjOxrwpVe+JpJsF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}